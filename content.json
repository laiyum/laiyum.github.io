{"meta":{"title":"玉米的博客","subtitle":null,"description":null,"author":"玉米","url":"http://laiyum.github.io"},"pages":[{"title":"categories","date":"2018-03-25T10:18:44.000Z","updated":"2018-03-25T10:19:23.541Z","comments":true,"path":"categories/index.html","permalink":"http://laiyum.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-03-25T10:03:44.000Z","updated":"2018-05-28T11:39:56.193Z","comments":true,"path":"tags/index.html","permalink":"http://laiyum.github.io/tags/index.html","excerpt":"","text":""},{"title":"无题","date":"2018-05-28T11:50:37.000Z","updated":"2018-06-09T05:08:47.522Z","comments":true,"path":"about/index.html","permalink":"http://laiyum.github.io/about/index.html","excerpt":"","text":"小时候，大人们总喜欢问我们，长大后想干什么，小朋友们总有各种各样的回答，到今天，我仍然记得我的回答是当一个历史学家，现在回过头来看，也许那时候是我离这个梦想最近的时刻。 人类似乎很喜欢寻找意义，许多的鸡汤也告诉你要过有意义的生活，顺便鄙夷下在他们看来没有意义的生活，在我看来，这本身就很可笑，且不说，每个人的人生不同无法对意义做出评判，况且，人的一生，不过是宇宙的一瞬，鸡汤们的有意义到头来，或许连历史的尘埃都不算。不过这世上也确实存在着有意义的生活，而人类所追求的有意义，也可归纳为两个问题，为何而生，为何而死。想起寻梦环游记里面的一句话，死亡不是终结，遗忘才是。当这世上没有人记得你了，你也就永远消失了，所以我更喜欢把寻找意义看成是寻找存在感，所谓人过留名，雁过留声，正因为人生是如此的短暂，人类才想方设法让自己被历史记住，或流芳百世，或遗臭万年。突然也就明白了小朋友们为什么热衷于当各种“家”了，也只有带这个头衔的才更容易被历史记住，看来让自己在这世上留存更久一点是人类与生俱来的本能了。 不过，真正名留史书的人，当初也不是为了青史留名，毛泽东带着红军长征是为了中国的未来，文天祥舍身就义也没想着身后受万人敬仰，他们不过是跟随自己的心而已。当然我要感谢名留青史的人，也要感谢为他们著书立传的人，正因为他们，我才知道，我脚下的土地，诸葛丞相曾在此誓师北伐；我眼前的关隘，六国曾在此逡巡而不敢前；我身后的高山是徐霞客历经艰险才登顶的；也正因为他们的传承，我们至今还能明白“求之不得，辗转反侧”的煎熬；体会“大漠孤烟直，长河落日圆”的壮阔；也沉醉于“江流宛转绕芳甸，月照花林皆似霰”的美景；唏嘘于“可怜无定河边骨，犹是春闺梦里人”的悲壮。这些被历史所记住的人，在一定程度上也获得了永生，不是吗？ 当然，我也是不能免俗的，总要留下些东西，才能证明你来过，总不能“山回路转不见君，雪上亦无马行处”吧 你好，我是玉米，爱敲代码，爱写诗词，感谢关注博客首页|微博|GitHub个人公众号：玉米的诗词杂谈"}],"posts":[{"title":"图像的卷积","slug":"图像的卷积","date":"2018-06-02T17:56:47.000Z","updated":"2018-06-02T18:05:04.449Z","comments":true,"path":"2018/06/03/图像的卷积/","link":"","permalink":"http://laiyum.github.io/2018/06/03/图像的卷积/","excerpt":"前言说到卷积，我想大多数人想到的是信号系统里面的卷积还有傅里叶变换，没错图像的卷积和信号系统里面的卷积非常类似，只不过前者是空间域上的，后者是时间域上的。看到卷积傅里叶，大家就本能的头疼对不对，没关系，本文将尽量摈弃复杂的数学公式，力求通俗易懂的解释图像的卷积。","text":"前言说到卷积，我想大多数人想到的是信号系统里面的卷积还有傅里叶变换，没错图像的卷积和信号系统里面的卷积非常类似，只不过前者是空间域上的，后者是时间域上的。看到卷积傅里叶，大家就本能的头疼对不对，没关系，本文将尽量摈弃复杂的数学公式，力求通俗易懂的解释图像的卷积。 卷积介绍卷积定义我们先来看看信号系统里面的卷积：其实质就是延迟加权求和，如下图所示： 我们可以看到，这是在时间域上的平移，而我们图像则是空间域上的卷积。 用一个卷积核去扫图像，图像对应部分和卷积核相乘后的矩阵求和作为新的值填充 卷积与滤波所谓滤波，就是消除不需要的频率成分,那么如何消除呢，我们知道傅里叶变换有个性质，时域卷积，频域相乘。如图，图一和图二分别是f(t)和g(t)的频域图，图三是f(t)*g(t)的过程，频域相乘，得到图四的结果，这个例子相当于一个低通滤波器。 图像滤波要解释图像的滤波，我们先要明白，图像在计算机里面表示是用二维矩阵的形式对应像素点的位置填充的数值为该点RGB的值，而RGB有三个值，所以呢我们就有三张图，称为通道。现在我们设计一个如下的滤波器，它能识别右边的曲线假设上面的核（滤波器）按照卷积顺序沿着下图移动那么当它移动到上面的位置时，按照矩阵操作，将这个区域的图像像素值与滤波器相乘，我们得到一个很大的值（6600）而当这个滤波器移动到其他区域时，我们得到一个相对很小的值如此，我们就对原图进行了一次卷积，得到的结果中，在那个特定曲线和周边区域，值就很高，在其他区域，值相对低。这就是一张激活图。对应的高值区域就是我们所要检测曲线的位置。 小结本文介绍的图像的卷积，在卷积神经网络中具有相当大的作用，用于图像特征的提取，我在后面对卷积神经网络也进行了详细的介绍。","categories":[],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://laiyum.github.io/tags/神经网络/"},{"name":"图像处理","slug":"图像处理","permalink":"http://laiyum.github.io/tags/图像处理/"}]},{"title":"卷积神经网络（CNN）详解","slug":"cnn","date":"2018-06-02T17:21:25.000Z","updated":"2018-06-03T10:34:57.065Z","comments":true,"path":"2018/06/03/cnn/","link":"","permalink":"http://laiyum.github.io/2018/06/03/cnn/","excerpt":"前言卷积神经网络（CNN）是一种著名的深度学习模型，其名称的由来是因为卷积运算被引入到了这种模型中，CNN可以归类为多层前馈神经网络模型（BP神经网络模型）在计算机视觉领域中得到了非常广泛的应用，例如图像分类、目标检测及跟踪等。下面，将详细介绍CNN的基本原理。","text":"前言卷积神经网络（CNN）是一种著名的深度学习模型，其名称的由来是因为卷积运算被引入到了这种模型中，CNN可以归类为多层前馈神经网络模型（BP神经网络模型）在计算机视觉领域中得到了非常广泛的应用，例如图像分类、目标检测及跟踪等。下面，将详细介绍CNN的基本原理。 why CNNCNN的输入是二维模型，所以利于处理图像视频的高维数据，同时由于其不用手动提取特征，使得在实际应用中可以大展身手，不过由于模型中所需参数极多，对计算机的计算力要求较高，加上训练模型所需要的数据量很大，所以在上世纪九十年代提出之后，基本没有得到大规模应用。随着大数据时代的到来，在高性能计算平台上用大规模的数据集训练复杂模型，成为了可能，正是在这种背景下，卷积神经网络开始蓬勃发展 CNN基本结构卷积神经网络顾名思义就是：卷积+神经网络，前面的博客中我已经详细介绍了图像卷积和BP神经网络，所以阅读这篇博客的读者，我就默认你们已经阅读过了前面的内容，还木有了解的小伙伴赶快点击上面链接。先来看一下CNN的基本结构： 主要有卷积层，池化层，全连接层，输出层，另外还涉及两个函数，激活函数和softmax函数，下面将一一进行介绍。 卷积层 我们知道传统的机器学习方法中，我们要手动进行特征选择和特征提取，而在CNN中是不需要我们提取特征，CNN会自动进行特征提取，那么CNN是怎么做到的呢，对了，特征提取的工作就由卷积层负责。我们在图像卷积那里提到，卷积其实就是滤波，滤波和神经网络又有什么关系呢？我们设想一下，如要识别图像中某种特定曲线，是不是就要用滤波器将其他曲线信号给抑制，让这种曲线经过滤波器时有很高的输出，这其实就是卷积层的特征提取过程了（详细过程见图像的卷积）这种滤波器，我们称为卷积核，一个卷积核只负责提取一种特征，卷积层一般又多个卷积核组成。 其过程动图如下： 对于卷积层，其输入是原始图像，输出是三个通道的特征图，原始图像和特征图维数的关系如下：(W−F+2P)/S+1其中W:输入单元的大小（宽或高）F:卷积核的大小S：步幅（stride）P：补零（zero-padding)的数量 池化层池化层的作用用一句话概括就是：采样降维。试想一下，如果用传统神经网络的全连接方式，对于上面28X28的特征图就需要28^4个参数，这还只是一个通道的一个特征图，显而易见，我们需要先降维。先将特征图划分小区域（一般为2X2小区域），然后用区域内最大值代替此区域，这就是最大池化；或用区域均值代替，这就是均值池化。如下图所示： 全连接层在前面，我们提到过，在传统神经网络中，我们使用的就是全连接，在卷积神经网络中，全连接层又有什么用呢？先来看全连接层的结构图： 我们可以看到，全连接层其实就是将多维特征图展开成一维，为什么要这么做呢，我们来看下面几张图: 他们都是猫但猫的位置和角度都不同，那么输入矩阵和特征矩阵肯定也是不一样的，但是我们的神经网络还是能识别出来，这就是全连接层做的，全连接层实际上呢识别前面提取到的特征进行组合，也就是说含有一维特征集中只要含有某些特征，就判断成猫，跟它的位置没有关系。 输出层与softmax函数 输出层神经元个数是与类别个数相一致的，因为我们的网络最终要依靠输出层的值来判断类别，但是输出层输出数值啊，如何根据数值判断类别呢，答案就是把数值转化成概率，概率大的类别即为神经网络最终的输出，这就是softmax函数的作用啦。 好了，到这里，整个神经网络的基本结构就介绍完了，慢着是不是感觉还缺少点什么，我们的神经网络是不是还是有点简单呢，对了，少了个激活函数啊。 激活函数在卷积层输出，一般会加上个激活函数，激活函数是用来干什么的呢一句话，激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。 这是个单层的感知机，显而易见的是，它只能执行线性分类任务， 这时候，我们加入激活函数，使其具有非线性分类的能力， 不知道各位看到这里想到了什么，很像SVN里面的核函数对不对，对了，他们的作用是一样的，引入非线性因素。 值得注意的是激活函数一般放在卷积层的输出，主要的激活函数有tanh, sigmoid和reLu等。 模型训练模型训练其实就是调参，所有参数初始阶段都是随机的，后面根据输出值和目标值之间的差值进行误差反向传播来调整参数值，详细介绍见BP神经网络 小结以上我们介绍的只是卷积神经网络的基本结构，实际上卷积核数量和其他各层数量，理论上都是可以任意添加的，这个可以根据具体任务来确定。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://laiyum.github.io/tags/机器学习/"},{"name":"神经网络","slug":"神经网络","permalink":"http://laiyum.github.io/tags/神经网络/"}]},{"title":"一文带你看懂BP神经网络","slug":"一文带你看懂BP神经网络","date":"2018-06-02T15:57:36.000Z","updated":"2018-06-02T16:14:54.539Z","comments":true,"path":"2018/06/02/一文带你看懂BP神经网络/","link":"","permalink":"http://laiyum.github.io/2018/06/02/一文带你看懂BP神经网络/","excerpt":"前言所谓BP神经网络，即前向反馈神经网络。我相信大多数人深度学习都是从神经网络开始的，本文将带大家了解神经网络基本模型结构，并详细介绍神经网络中最重要的算法，BP算法。","text":"前言所谓BP神经网络，即前向反馈神经网络。我相信大多数人深度学习都是从神经网络开始的，本文将带大家了解神经网络基本模型结构，并详细介绍神经网络中最重要的算法，BP算法。 神经网络我们先来看一下一个经典的神经网络结构模型：由三层神经元组成，好，图先放着，它的具体结构和工作原理，且听我慢慢道来。 神经元模型神经元神经网络中最基础的组成成分，结构如下图所示 连接 是神经元中最重要的部分，神经网络训练，就是调整连接上的权值，使得输出尽量接近目标值，在图中，神经元输出之前还要经过一个激活函数，负责将值转换成0或1，理想的激活函数是阶跃函数，但由于其不连续，不光滑，故而一般采用的是sigmod函数，如下图所示 神经元计算公式如下 : y=sgn(w*x+b),其中w为权值，b为偏置，是神经元的阈值。 感知机与多层网络多个神经元按一定层级组织起来就形成了神经网络。值得注意的是每一层神经元之间都是全连接。感知机的公式可以用如下公式表示： BP算法好，神经网络我们搭建好了，怎么样来训练参数呢，就要用到这个BP算法，即误差反向传播算法。其核心就是一句话：链式求导法则应用求误差最小值具体过程如下：这里为了简单，先忽略激活函数1.正向传播 2.误差反向传播 3.权值更新为什么要对W1进行权值更新呢，因为误差是关于W1的函数，要降低误差就要对W1进行更新。注意：权值更新这里涉及梯度下降，欢迎阅读了解详细 4.重复123 小结作为神经网络的精髓，本文详细介绍了BP算法过程，由于还不会用LaTeX，采用手写，后面将使用LaTeX更新这些公式。 参考：https://www.cnblogs.com/charlotte77/p/5629865.html","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://laiyum.github.io/tags/机器学习/"},{"name":"神经网络","slug":"神经网络","permalink":"http://laiyum.github.io/tags/神经网络/"}]},{"title":"机器学习入门","slug":"机器学习入门","date":"2017-05-28T11:24:50.000Z","updated":"2019-07-08T13:50:29.145Z","comments":true,"path":"2017/05/28/机器学习入门/","link":"","permalink":"http://laiyum.github.io/2017/05/28/机器学习入门/","excerpt":"前言近年来，机器学习人工智能领域大热，本文也来蹭蹭热点，接下来将为大家理清机器学习相关概念，介绍机器学习的主要方法","text":"前言近年来，机器学习人工智能领域大热，本文也来蹭蹭热点，接下来将为大家理清机器学习相关概念，介绍机器学习的主要方法 机器学习概念什么是机器学习我们通常在计算机编程的时候，都是已知输入，然后通过一定的算法产生输出，如下图所示： 而还有一种情况呢，我们知道了输入和输出，但是却不知道中间的算法，这时候就需要用机器学习通过观察输入输出来学习这个算法，我们叫做知识，然后用学到的知识通过新的输入来产生输出。简而言之机器学习是用来寻找输入输出间的映射关系的 其实我们人在学习思考时，脑子里也有这么个映射过程，当我们看到猫的图片，在脑子里产生了映射，于是乎，就说出了猫这个词，所以呢机器学习就是让计算机和人一样思考 机器学习相关概念的区别在谈到机器学习时，我们经常会听到下面几个概念：数据挖掘，人工智能等等，那他们之间有什么联系呢？我们先来看看下面的这幅图： 也就是说机器学习是数据挖掘它们的底层，机器学习给它们提供算法，数据挖掘则是机器学习的应用。 机器学习的应用像我们熟知的Apple的faceID，google的AlphaGo，MicroSoft的小娜都是机器学习的典型应用 机器学习方法 实现机器学习有多种方法，这里方法是我们通常所说的算法，主要有下面几种： 监督学习方法所谓监督学习方法就是说随便给一堆数据和数据对应的标签，计算机在学习之后能根据新的输入数据判断它们对应的标签。 举个例子，我们给计算机一大堆猫狗的图片，并告诉它，哪些是狗，哪些是猫，计算机学习之后，再给它看猫和狗的图片，它就能告诉我们哪些是狗，哪些是猫。 监督学习方法用于分类和回归，我们熟知的神经网络就是一种监督学习方法。 非监督学习方法我们有的时候只给计算机数据，不给计算机对应的标签，计算机能通过观察数据之间的规率进行数据归类还是上面的例子，我们这次不告诉计算机哪些是狗，哪些是猫，计算机学习之后能把他们归类出来。 非监督学习用于聚类，如k-means算法 半监督学习方法半监督学习方法综合了监督学习和非监督学习两种方法。在初始阶段给计算机一些有标签的数据和大量没有标签的数据，计算机学习后能进行归类半监督学习用于训练更高效更准确的模型，因为它既避免了带标签的数据少而的模型过拟合，也减少了打标签的工作量。 强化学习强化学习是学习一个最优策略，可以让本体在特定环境中，根据当前的状态，做出行动，从而获得最大奖励。 来个新例子，这次，我们让计算机打篮球，我们并不需要告诉计算机怎么打篮球，我们只需要给它个篮球，让它自己打，然后我们对计算机打分，进球分越高，打分越高，这里的打分就是上面的奖励，每次计算机要做的就是获得最大奖励，一开始，面对陌生的环境，计算机并不知道怎么进球，经过奖励刺激后，命中率就会越来越高。 google的AlphaZero便是强化学习的典型应用 遗传算法模拟自然界优胜劣汰的进化现象，把搜索空间（问题解的组成空间）映射为遗传空间，把可能的解编码成一个向量——染色体，向量的每个元素称为基因。通过不断计算各染色体的适应值，选择最好的染色体，获得最优解。 还是拿上面的例子来说，先训练出两个打篮球的计算机ai，让这两个aic重组，变异产生后代种群，挑出最会打篮球的再进行重组变异，如此循环，每次挑出最强的","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://laiyum.github.io/tags/机器学习/"},{"name":"编程","slug":"编程","permalink":"http://laiyum.github.io/tags/编程/"}]}]}